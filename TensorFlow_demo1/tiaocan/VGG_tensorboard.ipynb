{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG_tensorboard",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "HiBUJy0ifSCp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "60762251-e709-49f3-881c-aae77b95fee5"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 345.2MB 84kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.13.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.7)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.2)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.13.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu) (2.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu) (40.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu) (0.15.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu) (5.1.3)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k7av-ACLgtWa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "be82e4e5-42b3-4d08-c38b-a64edf58526a"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131304 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2bCCxw6FhaAZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZZ9s_Fi8hvpW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"drive/colab\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a9oMNZSqmdIV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "ecab0180-4daf-4ff0-8f67-7efc4ce0eb74"
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-22 15:19:57--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.203.102.189, 52.21.103.149, 35.172.177.65, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.203.102.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14977695 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  14.28M  9.02MB/s    in 1.6s    \n",
            "\n",
            "2019-04-22 15:20:03 (9.02 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [14977695/14977695]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lYO8Rr_yh4ZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3716
        },
        "outputId": "4a102ce7-c212-48aa-fc32-e3550e8fcea0"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import pickle\n",
        "import numpy as  np\n",
        "\n",
        "CIFAR_DIR = \"cifar-10-batches-py\"\n",
        "print(os.listdir(CIFAR_DIR))\n",
        "\n",
        "#thensorBoard\n",
        "#1,指定面板图上显示的变量\n",
        "#2.在训练过程中将这些变量值计算出来写到文件中\n",
        "#3，文件解析 ./tensorboard -- logdir =dir\n",
        "\n",
        "def load_data(filename):\n",
        "    \"\"\" read data from data file.\"\"\"\n",
        "    with open(filename,'rb') as f:\n",
        "        data = pickle.load(f ,encoding='latin1') #,encoding='latin1',encoding='iso-8859-1'，encoding='bytes'\n",
        "        return data['data'],data['labels']\n",
        "# tensorflow.dataset(可以使用)\n",
        "class cifardata:\n",
        "    def __init__(self,filenames,need_shuffle):\n",
        "        all_data = []\n",
        "        all_labels=[]\n",
        "        for filename in filenames:\n",
        "            data, labels = load_data(filename)\n",
        "            all_data.append(data)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "        self._data =np.vstack(all_data)\n",
        "        self._data = self._data/127.5 - 1 #数据归一化\n",
        "        self._labels = np.hstack(all_labels)\n",
        "        self._num_examples = self._data.shape[0]\n",
        "        self._need_shuffle = need_shuffle\n",
        "        self._indicator = 0\n",
        "        if self._need_shuffle:\n",
        "            self._shuffle_data()\n",
        "    def _shuffle_data(self):\n",
        "        p = np.random.permutation(self._num_examples)\n",
        "        self._data = self._data[p]\n",
        "        self._labels = self._labels[p]\n",
        "    def next_batch(self,batch_size):\n",
        "        '''return batch_size examples as a batch.'''\n",
        "        end_indicator = self._indicator + batch_size\n",
        "        if end_indicator > self._num_examples:\n",
        "            if self._need_shuffle:\n",
        "                self._shuffle_data()\n",
        "                self._indicator = 0\n",
        "                end_indicator = batch_size\n",
        "            else:\n",
        "                raise Exception(\"have no more examples\")\n",
        "        if end_indicator > self._num_examples:\n",
        "            raise Exception(\"batch size is larger than all examples\")\n",
        "        batch_data = self._data[self._indicator:end_indicator]\n",
        "        batch_lables = self._labels[self._indicator:end_indicator]\n",
        "        self._indicator = end_indicator\n",
        "        return batch_data,batch_lables\n",
        "\n",
        "train_filenames = [os.path.join(CIFAR_DIR,'data_batch_%d' % i ) for i in range(1,6)]\n",
        "test_filenames  = [os.path.join(CIFAR_DIR,'test_batch')]\n",
        "\n",
        "train_data = cifardata(train_filenames,True)\n",
        "test_data = cifardata(test_filenames,False)\n",
        "\n",
        "\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,3072])\n",
        "y = tf.placeholder(tf.int64,[None])\n",
        "x_image = tf.reshape(x,[-1,3,32,32])\n",
        "x_image = tf.transpose(x_image,perm=[0,2,3,1])\n",
        "#feature_map ,输出图 ，神经元图\n",
        "conv1_1 = tf.layers.conv2d(x_image,\n",
        "                         32, #out channel number\n",
        "                         (3,3),#kernal_size\n",
        "                         padding='same',\n",
        "                         activation=tf.nn.relu,\n",
        "                         name = 'conv1_1'\n",
        "                         )\n",
        "conv1_2 = tf.layers.conv2d(conv1_1,\n",
        "                         32, #out channel number\n",
        "                         (3,3),#kernal_size\n",
        "                         padding='same',\n",
        "                         activation=tf.nn.relu,\n",
        "                         name = 'conv1_2'\n",
        "                         )\n",
        "#16*16\n",
        "pooling1 = tf.layers.max_pooling2d( conv1_2,\n",
        "                                    (2,2),#kernal_size\n",
        "                                    (2,2),#stride\n",
        "                                    name='pool1'\n",
        "                                     )\n",
        "conv2_1 = tf.layers.conv2d(pooling1,\n",
        "                         32, #out channel number\n",
        "                         (3,3),#kernal_size\n",
        "                         padding='same',\n",
        "                         activation=tf.nn.relu,\n",
        "                         name = 'conv2_1'\n",
        "                         )\n",
        "conv2_2 = tf.layers.conv2d(conv2_1,\n",
        "                         32, #out channel number\n",
        "                         (3,3),#kernal_size\n",
        "                         padding='same',\n",
        "                         activation=tf.nn.relu,\n",
        "                         name = 'conv2_2'\n",
        "                         )\n",
        "#8*8\n",
        "pooling2 = tf.layers.max_pooling2d( conv2_2,\n",
        "                                    (2,2),#kernal_size\n",
        "                                    (2,2),#stride\n",
        "                                    name='pool2'\n",
        "                                     )\n",
        "conv3_1 = tf.layers.conv2d(pooling2,\n",
        "                         32, #out channel number\n",
        "                         (3,3),#kernal_size\n",
        "                         padding='same',\n",
        "                         activation=tf.nn.relu,\n",
        "                         name = 'conv3_1'\n",
        "                         )\n",
        "conv3_2 = tf.layers.conv2d(conv3_1,\n",
        "                         32, #out channel number\n",
        "                         (3,3),#kernal_size\n",
        "                         padding='same',\n",
        "                         activation=tf.nn.relu,\n",
        "                         name = 'conv3_2'\n",
        "                         )\n",
        "#4*4\n",
        "pooling3 = tf.layers.max_pooling2d( conv3_2,\n",
        "                                    (2,2),#kernal_size\n",
        "                                    (2,2),#stride\n",
        "                                    name='pool3'\n",
        "                                     )\n",
        "#[none , 4*4*32]\n",
        "flatten = tf.layers.flatten(pooling3)\n",
        "y_ = tf.layers.dense(flatten,10)\n",
        "\n",
        "\n",
        "loss = tf.losses.sparse_softmax_cross_entropy(labels=y,logits=y_)\n",
        "#y_ ->sofmax\n",
        "#y ->one_hot\n",
        "#loss->ylogy_\n",
        "predict = tf.argmax(y_,1)\n",
        "correct_prediction = tf.equal(predict,y)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float64))\n",
        "with tf.name_scope('train_op'):\n",
        "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
        "\n",
        "# def variable_summary(var,name):\n",
        "#      with tf.name_scope(name):\n",
        "#          mean = tf.reduce_mean(var)\n",
        "#          with tf.name_scorp('stddev'):\n",
        "#              stddev =tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "#          tf.summary.scalar('mean',mean)\n",
        "#          tf.summary.scalar('stddev',stddev)\n",
        "#          tf.summary.scalar('min',tf.reduce_min(var))\n",
        "#          tf.summary.scalar('max',tf.reduce_max(var))\n",
        "#          tf.summary.scalar('histogram',var)\n",
        "#\n",
        "# with tf.name_scope('summary'):\n",
        "#     variable_summary(conv1_1,'conv1_1')\n",
        "#     variable_summary(conv1_2,'conv1_2')\n",
        "#     variable_summary(conv2_1,'conv2_1')\n",
        "#     variable_summary(conv2_2,'conv2_2')\n",
        "#     variable_summary(conv3_1,'conv3_1')\n",
        "#     variable_summary(conv3_2,'conv3_2')\n",
        "\n",
        "\n",
        "loss_summary = tf.summary.scalar('loss',loss)\n",
        "accuracy_summary = tf.summary.scalar('accuracy',accuracy)\n",
        "source_image =(x_image +1 )*127.5\n",
        "inputs_summary = tf.summary.image('input_image',source_image)\n",
        "merged_summary = tf.summary.merge_all()\n",
        "merged_summary_test = tf.summary.merge([loss_summary,accuracy_summary])\n",
        "\n",
        "LOG_DIR='.'\n",
        "run_label = 'run_vgg_tensorboard'\n",
        "run_dir = os.path.join(LOG_DIR,run_label)\n",
        "if not os.path.exists(run_dir):\n",
        "    os.mkdir(run_dir)\n",
        "train_log_dir = os.path.join(run_dir,'train')\n",
        "test_log_dir = os.path.join(run_dir,'test')\n",
        "if not os.path.exists(train_log_dir):\n",
        "    os.mkdir(train_log_dir)\n",
        "if not os.path.exists(test_log_dir):\n",
        "    os.mkdir(test_log_dir)\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "batch_size = 20\n",
        "train_steps = 10000\n",
        "test_steps = 100\n",
        "output_summary_every_steps = 100\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    train_writer = tf.summary.FileWriter(train_log_dir,sess.graph)\n",
        "    test_writer = tf.summary.FileWriter(test_log_dir)\n",
        "\n",
        "    fixed_test_batch_data,fixed_test_batch_labels \\\n",
        "       = test_data.next_batch(batch_size)\n",
        "    for i in range(train_steps):\n",
        "        batch_data,batch_labels = train_data.next_batch(batch_size)\n",
        "        eval_ops = [loss,accuracy,train_op]\n",
        "        should_output_summary =((1+i) % output_summary_every_steps ==0)\n",
        "        if should_output_summary:\n",
        "            eval_ops.append(merged_summary)\n",
        "\n",
        "        eval_ops_results = sess.run(\n",
        "            eval_ops,\n",
        "            feed_dict={\n",
        "                x:batch_data ,\n",
        "                y:batch_labels })\n",
        "        loss_val,acc_val = eval_ops_results[0:2]\n",
        "        if should_output_summary:\n",
        "            train_summary_str = eval_ops_results[-1]\n",
        "            train_writer.add_summary(train_summary_str,i+1)\n",
        "            test_summary_str = sess.run([merged_summary_test],\n",
        "                                        feed_dict={\n",
        "                                            x:fixed_test_batch_data,\n",
        "                                            y:fixed_test_batch_labels,\n",
        "                                        })[0]\n",
        "            test_writer.add_summary(test_summary_str,i+1)\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print('[train] step: %d ,loss:%4.5f, acc: %4.5f' %(i,loss_val,acc_val))\n",
        "        if (i+1) % 100 == 0:\n",
        "            test_data = cifardata(test_filenames, False)\n",
        "            all_test_acc_val = [ ]\n",
        "            for j in range(test_steps):\n",
        "                test_batch_data, test_batch_labels \\\n",
        "                    = test_data.next_batch(batch_size)\n",
        "                test_acc_val = sess.run(\n",
        "                    [accuracy],\n",
        "                    feed_dict={\n",
        "                        x:test_batch_data,\n",
        "                        y:test_batch_labels})\n",
        "                all_test_acc_val.append(test_acc_val)\n",
        "            test_acc = np.mean(all_test_acc_val)\n",
        "            print('[test]step: %d ,acc:%4.5f '%(i+1,test_acc ) )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['batches.meta', 'readme.html', 'data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5', 'test_batch']\n",
            "WARNING:tensorflow:From <ipython-input-6-5b9da7dfab5a>:76: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-6-5b9da7dfab5a>:89: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.max_pooling2d instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-5b9da7dfab5a>:132: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-5b9da7dfab5a>:133: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "[train] step: 99 ,loss:2.21124, acc: 0.00000\n",
            "[test]step: 100 ,acc:0.26000 \n",
            "[train] step: 199 ,loss:1.43547, acc: 0.55000\n",
            "[test]step: 200 ,acc:0.33500 \n",
            "[train] step: 299 ,loss:1.58849, acc: 0.45000\n",
            "[test]step: 300 ,acc:0.34800 \n",
            "[train] step: 399 ,loss:1.68459, acc: 0.30000\n",
            "[test]step: 400 ,acc:0.37700 \n",
            "[train] step: 499 ,loss:1.94276, acc: 0.25000\n",
            "[test]step: 500 ,acc:0.41100 \n",
            "[train] step: 599 ,loss:1.25331, acc: 0.50000\n",
            "[test]step: 600 ,acc:0.41950 \n",
            "[train] step: 699 ,loss:1.72073, acc: 0.30000\n",
            "[test]step: 700 ,acc:0.45050 \n",
            "[train] step: 799 ,loss:1.46948, acc: 0.50000\n",
            "[test]step: 800 ,acc:0.46000 \n",
            "[train] step: 899 ,loss:1.30958, acc: 0.45000\n",
            "[test]step: 900 ,acc:0.49150 \n",
            "[train] step: 999 ,loss:1.25691, acc: 0.50000\n",
            "[test]step: 1000 ,acc:0.50250 \n",
            "[train] step: 1099 ,loss:1.49073, acc: 0.45000\n",
            "[test]step: 1100 ,acc:0.51150 \n",
            "[train] step: 1199 ,loss:1.37499, acc: 0.40000\n",
            "[test]step: 1200 ,acc:0.52300 \n",
            "[train] step: 1299 ,loss:1.12701, acc: 0.55000\n",
            "[test]step: 1300 ,acc:0.51450 \n",
            "[train] step: 1399 ,loss:1.42626, acc: 0.65000\n",
            "[test]step: 1400 ,acc:0.55950 \n",
            "[train] step: 1499 ,loss:1.52216, acc: 0.40000\n",
            "[test]step: 1500 ,acc:0.54150 \n",
            "[train] step: 1599 ,loss:1.58856, acc: 0.50000\n",
            "[test]step: 1600 ,acc:0.56400 \n",
            "[train] step: 1699 ,loss:1.20486, acc: 0.60000\n",
            "[test]step: 1700 ,acc:0.56450 \n",
            "[train] step: 1799 ,loss:1.17834, acc: 0.60000\n",
            "[test]step: 1800 ,acc:0.59300 \n",
            "[train] step: 1899 ,loss:0.95167, acc: 0.70000\n",
            "[test]step: 1900 ,acc:0.59850 \n",
            "[train] step: 1999 ,loss:1.06990, acc: 0.60000\n",
            "[test]step: 2000 ,acc:0.58100 \n",
            "[train] step: 2099 ,loss:1.08840, acc: 0.55000\n",
            "[test]step: 2100 ,acc:0.59300 \n",
            "[train] step: 2199 ,loss:1.25726, acc: 0.55000\n",
            "[test]step: 2200 ,acc:0.56000 \n",
            "[train] step: 2299 ,loss:1.28346, acc: 0.60000\n",
            "[test]step: 2300 ,acc:0.61500 \n",
            "[train] step: 2399 ,loss:1.38649, acc: 0.45000\n",
            "[test]step: 2400 ,acc:0.59350 \n",
            "[train] step: 2499 ,loss:1.06828, acc: 0.50000\n",
            "[test]step: 2500 ,acc:0.61250 \n",
            "[train] step: 2599 ,loss:0.98414, acc: 0.65000\n",
            "[test]step: 2600 ,acc:0.62150 \n",
            "[train] step: 2699 ,loss:1.78878, acc: 0.40000\n",
            "[test]step: 2700 ,acc:0.62850 \n",
            "[train] step: 2799 ,loss:0.91998, acc: 0.65000\n",
            "[test]step: 2800 ,acc:0.63250 \n",
            "[train] step: 2899 ,loss:0.83615, acc: 0.75000\n",
            "[test]step: 2900 ,acc:0.63200 \n",
            "[train] step: 2999 ,loss:1.04119, acc: 0.60000\n",
            "[test]step: 3000 ,acc:0.62300 \n",
            "[train] step: 3099 ,loss:1.23386, acc: 0.65000\n",
            "[test]step: 3100 ,acc:0.61900 \n",
            "[train] step: 3199 ,loss:1.18612, acc: 0.60000\n",
            "[test]step: 3200 ,acc:0.65400 \n",
            "[train] step: 3299 ,loss:1.41207, acc: 0.40000\n",
            "[test]step: 3300 ,acc:0.64000 \n",
            "[train] step: 3399 ,loss:0.82658, acc: 0.70000\n",
            "[test]step: 3400 ,acc:0.64050 \n",
            "[train] step: 3499 ,loss:0.90531, acc: 0.75000\n",
            "[test]step: 3500 ,acc:0.64200 \n",
            "[train] step: 3599 ,loss:1.14413, acc: 0.45000\n",
            "[test]step: 3600 ,acc:0.64750 \n",
            "[train] step: 3699 ,loss:1.00412, acc: 0.65000\n",
            "[test]step: 3700 ,acc:0.65800 \n",
            "[train] step: 3799 ,loss:1.02741, acc: 0.70000\n",
            "[test]step: 3800 ,acc:0.64900 \n",
            "[train] step: 3899 ,loss:0.69516, acc: 0.65000\n",
            "[test]step: 3900 ,acc:0.64350 \n",
            "[train] step: 3999 ,loss:0.96224, acc: 0.70000\n",
            "[test]step: 4000 ,acc:0.66250 \n",
            "[train] step: 4099 ,loss:0.61148, acc: 0.85000\n",
            "[test]step: 4100 ,acc:0.68050 \n",
            "[train] step: 4199 ,loss:0.81954, acc: 0.70000\n",
            "[test]step: 4200 ,acc:0.65550 \n",
            "[train] step: 4299 ,loss:1.00290, acc: 0.75000\n",
            "[test]step: 4300 ,acc:0.67750 \n",
            "[train] step: 4399 ,loss:1.03723, acc: 0.65000\n",
            "[test]step: 4400 ,acc:0.66800 \n",
            "[train] step: 4499 ,loss:0.92312, acc: 0.70000\n",
            "[test]step: 4500 ,acc:0.67000 \n",
            "[train] step: 4599 ,loss:0.93327, acc: 0.70000\n",
            "[test]step: 4600 ,acc:0.67050 \n",
            "[train] step: 4699 ,loss:0.93706, acc: 0.60000\n",
            "[test]step: 4700 ,acc:0.68200 \n",
            "[train] step: 4799 ,loss:1.05739, acc: 0.65000\n",
            "[test]step: 4800 ,acc:0.67850 \n",
            "[train] step: 4899 ,loss:1.05295, acc: 0.65000\n",
            "[test]step: 4900 ,acc:0.65800 \n",
            "[train] step: 4999 ,loss:1.19088, acc: 0.55000\n",
            "[test]step: 5000 ,acc:0.67400 \n",
            "[train] step: 5099 ,loss:1.04965, acc: 0.60000\n",
            "[test]step: 5100 ,acc:0.67700 \n",
            "[train] step: 5199 ,loss:0.43200, acc: 0.90000\n",
            "[test]step: 5200 ,acc:0.67650 \n",
            "[train] step: 5299 ,loss:0.70421, acc: 0.85000\n",
            "[test]step: 5300 ,acc:0.69400 \n",
            "[train] step: 5399 ,loss:0.60507, acc: 0.75000\n",
            "[test]step: 5400 ,acc:0.67400 \n",
            "[train] step: 5499 ,loss:1.20932, acc: 0.65000\n",
            "[test]step: 5500 ,acc:0.68950 \n",
            "[train] step: 5599 ,loss:1.44939, acc: 0.55000\n",
            "[test]step: 5600 ,acc:0.67650 \n",
            "[train] step: 5699 ,loss:0.64143, acc: 0.75000\n",
            "[test]step: 5700 ,acc:0.70300 \n",
            "[train] step: 5799 ,loss:0.35556, acc: 0.90000\n",
            "[test]step: 5800 ,acc:0.68300 \n",
            "[train] step: 5899 ,loss:0.98646, acc: 0.75000\n",
            "[test]step: 5900 ,acc:0.69800 \n",
            "[train] step: 5999 ,loss:0.59029, acc: 0.80000\n",
            "[test]step: 6000 ,acc:0.67350 \n",
            "[train] step: 6099 ,loss:1.26197, acc: 0.55000\n",
            "[test]step: 6100 ,acc:0.67800 \n",
            "[train] step: 6199 ,loss:0.57745, acc: 0.75000\n",
            "[test]step: 6200 ,acc:0.67850 \n",
            "[train] step: 6299 ,loss:0.67905, acc: 0.75000\n",
            "[test]step: 6300 ,acc:0.71450 \n",
            "[train] step: 6399 ,loss:0.84476, acc: 0.75000\n",
            "[test]step: 6400 ,acc:0.70500 \n",
            "[train] step: 6499 ,loss:0.57837, acc: 0.85000\n",
            "[test]step: 6500 ,acc:0.70800 \n",
            "[train] step: 6599 ,loss:1.35505, acc: 0.65000\n",
            "[test]step: 6600 ,acc:0.69600 \n",
            "[train] step: 6699 ,loss:1.34511, acc: 0.60000\n",
            "[test]step: 6700 ,acc:0.68750 \n",
            "[train] step: 6799 ,loss:0.92999, acc: 0.70000\n",
            "[test]step: 6800 ,acc:0.70300 \n",
            "[train] step: 6899 ,loss:0.72960, acc: 0.80000\n",
            "[test]step: 6900 ,acc:0.72050 \n",
            "[train] step: 6999 ,loss:1.01864, acc: 0.65000\n",
            "[test]step: 7000 ,acc:0.72050 \n",
            "[train] step: 7099 ,loss:0.70505, acc: 0.70000\n",
            "[test]step: 7100 ,acc:0.70600 \n",
            "[train] step: 7199 ,loss:0.67085, acc: 0.65000\n",
            "[test]step: 7200 ,acc:0.70550 \n",
            "[train] step: 7299 ,loss:0.87448, acc: 0.65000\n",
            "[test]step: 7300 ,acc:0.70700 \n",
            "[train] step: 7399 ,loss:0.70728, acc: 0.75000\n",
            "[test]step: 7400 ,acc:0.71800 \n",
            "[train] step: 7499 ,loss:0.43939, acc: 0.90000\n",
            "[test]step: 7500 ,acc:0.71500 \n",
            "[train] step: 7599 ,loss:0.45734, acc: 0.85000\n",
            "[test]step: 7600 ,acc:0.70650 \n",
            "[train] step: 7699 ,loss:0.76828, acc: 0.65000\n",
            "[test]step: 7700 ,acc:0.70300 \n",
            "[train] step: 7799 ,loss:0.80240, acc: 0.70000\n",
            "[test]step: 7800 ,acc:0.69950 \n",
            "[train] step: 7899 ,loss:0.46137, acc: 0.85000\n",
            "[test]step: 7900 ,acc:0.71200 \n",
            "[train] step: 7999 ,loss:0.76214, acc: 0.80000\n",
            "[test]step: 8000 ,acc:0.72300 \n",
            "[train] step: 8099 ,loss:0.77228, acc: 0.70000\n",
            "[test]step: 8100 ,acc:0.71650 \n",
            "[train] step: 8199 ,loss:0.89560, acc: 0.70000\n",
            "[test]step: 8200 ,acc:0.72150 \n",
            "[train] step: 8299 ,loss:0.64369, acc: 0.75000\n",
            "[test]step: 8300 ,acc:0.71050 \n",
            "[train] step: 8399 ,loss:0.84220, acc: 0.75000\n",
            "[test]step: 8400 ,acc:0.71800 \n",
            "[train] step: 8499 ,loss:0.36841, acc: 0.85000\n",
            "[test]step: 8500 ,acc:0.72600 \n",
            "[train] step: 8599 ,loss:0.71136, acc: 0.75000\n",
            "[test]step: 8600 ,acc:0.74200 \n",
            "[train] step: 8699 ,loss:0.63261, acc: 0.70000\n",
            "[test]step: 8700 ,acc:0.69400 \n",
            "[train] step: 8799 ,loss:1.08149, acc: 0.75000\n",
            "[test]step: 8800 ,acc:0.72050 \n",
            "[train] step: 8899 ,loss:0.78307, acc: 0.85000\n",
            "[test]step: 8900 ,acc:0.72550 \n",
            "[train] step: 8999 ,loss:0.77078, acc: 0.70000\n",
            "[test]step: 9000 ,acc:0.70100 \n",
            "[train] step: 9099 ,loss:0.82001, acc: 0.70000\n",
            "[test]step: 9100 ,acc:0.71850 \n",
            "[train] step: 9199 ,loss:0.61345, acc: 0.75000\n",
            "[test]step: 9200 ,acc:0.71900 \n",
            "[train] step: 9299 ,loss:1.10046, acc: 0.50000\n",
            "[test]step: 9300 ,acc:0.73600 \n",
            "[train] step: 9399 ,loss:1.13075, acc: 0.65000\n",
            "[test]step: 9400 ,acc:0.69750 \n",
            "[train] step: 9499 ,loss:0.63600, acc: 0.70000\n",
            "[test]step: 9500 ,acc:0.71800 \n",
            "[train] step: 9599 ,loss:1.59074, acc: 0.45000\n",
            "[test]step: 9600 ,acc:0.72000 \n",
            "[train] step: 9699 ,loss:0.73126, acc: 0.70000\n",
            "[test]step: 9700 ,acc:0.71000 \n",
            "[train] step: 9799 ,loss:0.28761, acc: 0.95000\n",
            "[test]step: 9800 ,acc:0.71350 \n",
            "[train] step: 9899 ,loss:0.47059, acc: 0.90000\n",
            "[test]step: 9900 ,acc:0.70550 \n",
            "[train] step: 9999 ,loss:0.64972, acc: 0.65000\n",
            "[test]step: 10000 ,acc:0.72400 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dMaXLELqjmCD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './run_vgg_tensorboard'\n",
        "get_ipython().system_raw(\n",
        "'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        ".format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DwEaTpXQm1r1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r16QtDAmncOn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd3e4673-80aa-467e-a8ac-3981185b868e"
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "\"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://baea12b3.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sFRliNyinCTr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}